[{"authors":null,"categories":null,"content":"The fight itself towards the summits suffices to fill a heart of man. Siwei Yang is a first-year Ph.D. student at UCSC supervised by Prof. Cihang Xie.\nHe received his Bachelor’s Degree in Computer Science at Tongji University, Shanghai, China. He was an research assistant supervised by Prof. Yin Wang at Tongji University. After graduation, he has worked as a visiting research assistant at MMLab@HKUST under the supervision of Prof. Dan Xu and a research summer intern at CCVL@JHU under the supervision of Prof. Alan Yuille and Prof. Hang Zhao.\nDownload my CV here for further information.\n","date":1707868800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1707868800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The fight itself towards the summits suffices to fill a heart of man. Siwei Yang is a first-year Ph.D. student at UCSC supervised by Prof. Cihang Xie.\nHe received his Bachelor’s Degree in Computer Science at Tongji University, Shanghai, China.","tags":null,"title":"Siwei Yang","type":"authors"},{"authors":["Siwei Yang","Bingchen Zhao","Cihang Xie"],"categories":null,"content":" ","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707868800,"objectID":"c2d145341bd1f03c88c5a5054c7d7a54","permalink":"https://laos-y.github.io/publication/yang2024aqabench/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/publication/yang2024aqabench/","section":"publication","summary":"This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning.","tags":["Large Language Model","Sequential Reasoning","Benchmark"],"title":"AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability","type":"publication"},{"authors":["Siwei Yang","Longlong Jing","Junfei Xiao","Hang Zhao","Alan Yuille","Yingwei Li"],"categories":null,"content":" ","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"a00396d0a17a0374207eb1d3c4058b64","permalink":"https://laos-y.github.io/publication/yang2022asyinst/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/publication/yang2022asyinst/","section":"publication","summary":"The weakly supervised instance segmentation is a challenging task. The existing methods typically use bounding boxes as supervision and optimize the network with a regularization loss term such as pairwise color affinity loss for instance segmentation. Through systematic analysis, we found that the commonly used pairwise affinity loss has two limitations: (1) it works with color affinity but leads to inferior performance with other modalities such as depth gradient, (2) the original affinity loss does not prevent trivial predictions as intended but actually accelerates this process due to the affinity loss term being symmetric. To overcome these two limitations, in this paper, we propose a novel asymmetric affinity loss which provides the penalty against the trivial prediction and generalizes well with affinity loss from different modalities. With the proposed asymmetric affinity loss, our method outperforms the state-ofthe-art methods on the Cityscapes dataset and outperforms our baseline method by 3.5% in mask AP.","tags":["Instance Segmentation","Weakly-supervised Learning","Multi-modal Vision"],"title":"AsyInst: Asymmetric Affinity with DepthGrad and Color for Box-Supervised Instance Segmentation","type":"publication"},{"authors":["Siwei Yang","Hangrong Ye","Dan Xu"],"categories":null,"content":" ","date":1669334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669334400,"objectID":"7939096699b3d4752de33e7f0c15c4f0","permalink":"https://laos-y.github.io/publication/yang2023aaai/","publishdate":"2023-06-26T00:00:00Z","relpermalink":"/publication/yang2023aaai/","section":"publication","summary":"This paper targets the problem of multi-task dense prediction which aims to achieve simultaneous learning and inference on a bunch of multiple dense prediction tasks in a single framework. A core objective in design is how to effectively model cross-task interactions to achieve a comprehensive improvement on different tasks based on their inherent complementarity and consistency. Existing works typically design extra expensive distillation modules to perform explicit interaction computations among different task-specific features in both training and inference, bringing difficulty in adaptation for different task sets, and reducing efficiency due to clearly increased size of multi-task models. In contrast, we introduce feature-wise contrastive consistency into modeling the crosstask interactions for multi-task dense prediction. We propose a novel multi-task contrastive regularization method based on the consistency to effectively boost the representation learning of the different sub-tasks, which can also be easily generalized to different multi-task dense prediction frameworks, and costs no additional computation in the inference. Extensive experiments on two challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the superiority of the proposed multi-task contrastive learning approach for dense predictions, establishing new state-of-the-art performances.","tags":["Multi-task Learning","Multi-modal Vision"],"title":"Contrastive Multi-Task Dense Prediction","type":"publication"},{"authors":["Yixin Fei","Zhongkai Zhao","Siwei Yang","Bingchen Zhao"],"categories":null,"content":" ","date":1668902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668902400,"objectID":"2b2902c00222312f489ed020e5409d67","permalink":"https://laos-y.github.io/publication/fei2022xcon/","publishdate":"2022-11-20T00:00:00Z","relpermalink":"/publication/fei2022xcon/","section":"publication","summary":"We address the problem of generalized category discovery (GCD) in this paper, i.e. clustering the unlabeled images leveraging the information from a set of seen classes, where the unlabeled images could contain both seen classes and unseen classes. The seen classes can be seen as an implicit criterion of classes, which makes this setting different from unsupervised clustering where the cluster criteria may be ambiguous. We mainly concern the problem of discovering categories within a fine-grained dataset since it is one of the most direct applications of category discovery, i.e. helping experts discover novel concepts within an unlabeled dataset using the implicit criterion set forth by the seen classes. State-of-the-art methods for generalized category discovery leverage contrastive learning to learn the representations, but the large inter-class similarity and intra-class variance pose a challenge for the methods because the negative examples may contain irrelevant cues for recognizing a category so the algorithms may converge to a local-minima. We present a novel method called Expert-Contrastive Learning (XCon) to help the model to mine useful information from the images by first partitioning the dataset into sub-datasets using k-means clustering and then performing contrastive learning on each of the sub-datasets to learn fine-grained discriminative features. Experiments on fine-grained datasets show a clear improved performance over the previous best methods, indicating the effectiveness of our method.","tags":["Category Discovery"],"title":"XCon: Learning with Experts for Fine-grained Category Discovery ","type":"publication"},{"authors":null,"categories":null,"content":"","date":1622592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622592000,"objectID":"dd88c67566cedd221f02fd85506299aa","permalink":"https://laos-y.github.io/project/cvpods/","publishdate":"2021-06-02T00:00:00Z","relpermalink":"/project/cvpods/","section":"project","summary":"A versatile and efficient codebase for multiple computer vision tasks.","tags":["Deep Learning"],"title":"Cvpods","type":"project"},{"authors":["Siwei Yang","Shaozuo Yu","Bingchen Zhao","Yin Wang"],"categories":null,"content":" ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"1591f71d0e067483700458b88db72767","permalink":"https://laos-y.github.io/publication/yang2020cvprw/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/publication/yang2020cvprw/","section":"publication","summary":"Visual pattern recognition over agricultural areas is an important application of aerial image processing. In this paper,weconsiderthemulti-modalitynatureofagricultural aerial images and show that naively combining different modalities together without taking the feature divergence intoaccountcanleadtosub-optimalresults. Thus,weapply a SwitchableNormalization blockto ourDeepLabV3+ segmentation model to alleviate the feature divergence. Using the popular symmetric Kullback–Leibler divergence measure, we show that our model can greatly reduce the divergence between RGB and near-infrared channels. Together with a hybrid loss function, our model achieves nearly 10% improvements in mean IoU over previously published baseline.","tags":["Semantic Segmentation"],"title":"Reducing the feature divergence of RGB and near-infrared images using Switchable Normalization","type":"publication"},{"authors":null,"categories":null,"content":"Code for AGRICULTURE-VISION 2020. Final ranking at the 8th place.\n","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"680c5fa6580a1817dad932eefa0d35bd","permalink":"https://laos-y.github.io/project/agrivision/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/project/agrivision/","section":"project","summary":"Code for [AGRICULTURE-VISION 2020](https://www.agrivision-workshop.com/?tdsourcetag=s_pctim_aiomsg). Final ranking at the 8th place.","tags":["Semantic Segmentation","Deep Learning"],"title":"AgriVision","type":"project"},{"authors":null,"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"7874b9d430341a348519559f1d821614","permalink":"https://laos-y.github.io/project/2-stage-egfr/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/project/2-stage-egfr/","section":"project","summary":"A 2-stage lung cancer classiﬁer baesd on CT images using Pytorch.","tags":["Medical Imaging","Deep Learning"],"title":"2-Stage EGFR","type":"project"},{"authors":null,"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"e7487f6912344e772dfcd9b0d7127f98","permalink":"https://laos-y.github.io/project/stanfordml-chexpert/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/project/stanfordml-chexpert/","section":"project","summary":"Code for [Stanford CheXpert competition](https://stanfordmlgroup.github.io/competitions/chexpert)","tags":["Medical Imaging","Deep Learning"],"title":"StanfordML-CheXpert","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://laos-y.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ad56ec238ab4dde3088bd4647990f0d8","permalink":"https://laos-y.github.io/project/real-time-ray-tracer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/real-time-ray-tracer/","section":"project","summary":"A solar system rendered by ray-tracing with OpenGL. Use Compute Shader written in GLSL for RT calculation..","tags":["Computer Graphics"],"title":"Real-time Ray Tracer","type":"project"}]